{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'de-akira'\n",
    "prefix = 'sagemaker/unsupervised_category'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from generate_example_data import generate_griffiths_data, plot_topic_data\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import scipy\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "import neologdn\n",
    "import os\n",
    "import urllib.request\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import MeCab\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#chaneg dummies to matrix\n",
    "from scipy.sparse import csr_matrix,coo_matrix\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import  roc_auc_score,f1_score\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate,Conv1D, MaxPool1D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau,Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import os\n",
    "import tarfile\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas(desc=\"hoge progress: \")\n",
    "\n",
    "import time\n",
    "import gensim\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = tokenizer.Tokenizer.SplitMode.A\n",
    "def wakati_by_sudachi(txt):\n",
    "    processed=[m.normalized_form() for m in tokenizer_obj.tokenize(txt, mode)]\n",
    "    return \" \".join(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize japanase words\n",
    "def wakati_by_mecab(text):\n",
    "    tagger = MeCab.Tagger('-Ochasen -d /usr/lib64/mecab/dic/mecab-ipadic-neologd')\n",
    "    tagger.parse('') \n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "        #if pos in [\"名詞\",\"動詞\",\"形容詞\",\"記号\"]:   # 対象とする品詞\n",
    "        if pos in [\"名詞\"]:   # 対象とする品詞\n",
    "            word = node.surface\n",
    "            word_list.append(word)\n",
    "        node = node.next\n",
    "    return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(row):\n",
    "    cust_list=['\\d+','■','\\n','#','◇','①','②','③','④','【】','】','【','☆','_','%','「','」','★','/']\n",
    "    del_list = string.ascii_letters+'\"#$%&\\'()*+,-./:;<=>@[\\\\]^_`{|}~'\n",
    "    for i in del_list:\n",
    "        row = row.str.replace(i,'')\n",
    "    for i in cust_list :\n",
    "        row = row.str.replace(i,'')\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text):\n",
    "    mapping ={\"矢張り\":\"やはり\",\"迚も\":\"とても\",\"迚も\":\"とても\",\"此れ\":\"これ\",\"其れ\":\"それ\",\"此の\":\"この\",\"此の\":\"この\",\"可成\":\"かなり\",\"兎に角\":\"とにかく\",\"態々\":\"わざわざ\"\n",
    "             ,\"です\":\"\",\"だ\":\"\",\"ます\":\"\",\"て\":\"\",\"た\":\"\"}\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel=pd.read_excel('/home/ec2-user/SageMaker/Notebooks_For_CX_Usecases/Unsupervised_Category/raw_data/gr_comment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel['comment'] = remove_chars(df_mutilabel['comment'])\n",
    "df_mutilabel['comment'].replace('', np.nan, inplace=True)\n",
    "df_mutilabel['comment']=df_mutilabel['comment'].apply(neologdn.normalize)\n",
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_mutilabel['comment'] = pool.map(wakati_by_mecab,df_mutilabel['comment'])\n",
    "pool.close() \n",
    "pool.join()\n",
    "\n",
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_mutilabel['comment'] = pool.map(clean_contractions,df_mutilabel['comment'])\n",
    "pool.close() \n",
    "pool.join()\n",
    "df_mutilabel['comment'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def count_len(s):\n",
    "    return len(s)\n",
    "df_mutilabel['comment'].apply(count_len).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nums of docs, voc size etc.\n",
    "num_documents = 1815\n",
    "num_topics = 10\n",
    "vocabulary_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel[\"comment\"].fillna(\"_##_\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df_mutilabel[\"comment\"].values\n",
    "tokenizer = Tokenizer(num_words=600)\n",
    "tokenizer.fit_on_texts(list(df_mutilabel['comment']))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "train_X = pad_sequences(train_X, padding=\"post\",maxlen=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# creating matrix\n",
    "input_matrix = train_X\n",
    "print('Input shape: ', input_matrix.shape)\n",
    "# splitting into two matrices of second matrix by size\n",
    "second_size = 300/1815\n",
    "\n",
    "train_x, val_x = train_test_split(input_matrix, test_size=second_size)\n",
    "\n",
    "print('X1 shape: ', train_x.shape)\n",
    "print('X2 shape: ', val_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_training = (train_x, np.zeros(int(len(train_x))))\n",
    "data_validation=(val_x, np.zeros(int(len(val_x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = (train_X, np.zeros(int(len(train_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First training document = {}'.format(train_X[0]))\n",
    "print('\\nVocabulary size = {}'.format(vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, data_training[0].astype('float32'))\n",
    "buf.seek(0)\n",
    "\n",
    "key = 'ntm.data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, data_validation[0].astype('float32'))\n",
    "buf.seek(0)\n",
    "\n",
    "key = 'ntm.data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/validation/{}'.format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "ntm.set_hyperparameters(num_topics=num_topics,\n",
    "                        feature_dim=vocabulary_size)\n",
    "\n",
    "ntm.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#real time to get predictor from endpoint\n",
    "ntm_predictor=sagemaker.predictor.RealTimePredictor(endpoint=\"ntm-endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ntm_predictor.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(listOfStr):\n",
    "    value=max(listOfStr)\n",
    "    dictOfWords = { i : listOfStr[i] for i in range(0, len(listOfStr) ) }\n",
    "    for index, num in dictOfWords.items():\n",
    "        if num == value:\n",
    "            return index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_list=[]\n",
    "for lst in range(len(predictions)):\n",
    "    cate_list.append(get_category(predictions[lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=pd.Series(cate_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\"comment\":df_mutilabel['comment'].tolist(),\"category\":s.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['category']==7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel=df_mutilabel['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel['category']=pd.Series(cate_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
