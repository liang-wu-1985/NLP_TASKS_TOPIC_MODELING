{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'de-akira'\n",
    "prefix = 'sagemaker/unsupervised_category'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from generate_example_data import generate_griffiths_data, plot_topic_data\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import scipy\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "import neologdn\n",
    "import os\n",
    "import urllib.request\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import MeCab\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#chaneg dummies to matrix\n",
    "from scipy.sparse import csr_matrix,coo_matrix\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import  roc_auc_score,f1_score\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate,Conv1D, MaxPool1D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau,Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import os\n",
    "import tarfile\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas(desc=\"hoge progress: \")\n",
    "\n",
    "import time\n",
    "import gensim\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = tokenizer.Tokenizer.SplitMode.A\n",
    "def wakati_by_sudachi(txt):\n",
    "    processed=[m.normalized_form() for m in tokenizer_obj.tokenize(txt, mode)]\n",
    "    return \" \".join(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize japanase words\n",
    "def wakati_by_mecab(text):\n",
    "    tagger = MeCab.Tagger('-Ochasen -d /usr/lib64/mecab/dic/mecab-ipadic-neologd')\n",
    "    tagger.parse('') \n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "        #if pos in [\"名詞\",\"動詞\",\"形容詞\",\"記号\"]:   # 対象とする品詞\n",
    "        if pos in [\"名詞\"]:   # 対象とする品詞\n",
    "            word = node.surface\n",
    "            word_list.append(word)\n",
    "        node = node.next\n",
    "    return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(row):\n",
    "    cust_list=['\\d+','■','\\n','#','◇','①','②','③','④','【】','】','【','☆','_','%','「','」','★','/']\n",
    "    del_list = string.ascii_letters+'\"#$%&\\'()*+,-./:;<=>@[\\\\]^_`{|}~'\n",
    "    for i in del_list:\n",
    "        row = row.str.replace(i,'')\n",
    "    for i in cust_list :\n",
    "        row = row.str.replace(i,'')\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text):\n",
    "    mapping ={\"矢張り\":\"やはり\",\"迚も\":\"とても\",\"迚も\":\"とても\",\"此れ\":\"これ\",\"其れ\":\"それ\",\"此の\":\"この\",\"此の\":\"この\",\"可成\":\"かなり\",\"兎に角\":\"とにかく\",\"態々\":\"わざわざ\"\n",
    "             ,\"です\":\"\",\"だ\":\"\",\"ます\":\"\",\"て\":\"\",\"た\":\"\"}\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel=pd.read_excel('/home/ec2-user/SageMaker/Notebooks_For_CX_Usecases/Unsupervised_Category/raw_data/gr_comment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel['comment'] = remove_chars(df_mutilabel['comment'])\n",
    "df_mutilabel['comment'].replace('', np.nan, inplace=True)\n",
    "df_mutilabel['comment']=df_mutilabel['comment'].apply(neologdn.normalize)\n",
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_mutilabel['comment'] = pool.map(wakati_by_mecab,df_mutilabel['comment'])\n",
    "pool.close() \n",
    "pool.join()\n",
    "\n",
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_mutilabel['comment'] = pool.map(clean_contractions,df_mutilabel['comment'])\n",
    "pool.close() \n",
    "pool.join()\n",
    "df_mutilabel['comment'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def count_len(s):\n",
    "    return len(s)\n",
    "df_mutilabel['comment'].apply(count_len).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nums of docs, voc size etc.\n",
    "num_documents = 1815\n",
    "num_topics = 10\n",
    "vocabulary_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel[\"comment\"].fillna(\"_##_\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df_mutilabel[\"comment\"].values\n",
    "tokenizer = Tokenizer(num_words=600)\n",
    "tokenizer.fit_on_texts(list(df_mutilabel['comment']))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "train_X = pad_sequences(train_X, padding=\"post\",maxlen=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1815, 50)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (1815, 50)\n",
      "X1 shape:  (1515, 50)\n",
      "X2 shape:  (300, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# creating matrix\n",
    "input_matrix = train_X\n",
    "print('Input shape: ', input_matrix.shape)\n",
    "# splitting into two matrices of second matrix by size\n",
    "second_size = 300/1815\n",
    "\n",
    "train_x, val_x = train_test_split(input_matrix, test_size=second_size)\n",
    "\n",
    "print('X1 shape: ', train_x.shape)\n",
    "print('X2 shape: ', val_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_training = (train_x, np.zeros(int(len(train_x))))\n",
    "data_validation=(val_x, np.zeros(int(len(val_x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = (train_X, np.zeros(int(len(train_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training document = [24  8 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "\n",
      "Vocabulary size = 50\n"
     ]
    }
   ],
   "source": [
    "print('First training document = {}'.format(train_X[0]))\n",
    "print('\\nVocabulary size = {}'.format(vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, data_training[0].astype('float32'))\n",
    "buf.seek(0)\n",
    "\n",
    "key = 'ntm.data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, data_validation[0].astype('float32'))\n",
    "buf.seek(0)\n",
    "\n",
    "key = 'ntm.data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/validation/{}'.format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://de-akira/sagemaker/unsupervised_category/train/ntm.data'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-26 11:03:17 Starting - Starting the training job...\n",
      "2019-08-26 11:03:19 Starting - Launching requested ML instances......\n",
      "2019-08-26 11:04:47 Starting - Preparing the instances for training......\n",
      "2019-08-26 11:05:42 Downloading - Downloading input data\n",
      "2019-08-26 11:05:42 Training - Downloading the training image...\n",
      "2019-08-26 11:06:18 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'50', u'num_topics': u'10'}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] Final configuration: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'feature_dim': u'50', u'weight_decay': u'0.0', u'num_topics': u'10', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] nvidia-smi took: 0.025269985199 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] Using default worker.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] None\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] vocab.txt\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:20 INFO 139999912929088] Vocab file is not provided\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1566817581.016117, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1566817581.016087}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.016] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 58368}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.071] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 54, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 1 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.92129036784\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.00630034244386\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.91499006748\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.92129036784\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=1.92129036784\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.06s, val: 0.00s, epoch: 0.06s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Total Records Seen\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1566817581.077539, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1566817581.016351}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=4893.65644215 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.103] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 25, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 2 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.85274863243\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.007950151572\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.8447984755\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.85274863243\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=1.85274863243\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}, \"Total Records Seen\": {\"count\": 1, \"max\": 600, \"sum\": 600.0, \"min\": 600}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1566817581.109051, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1566817581.077783}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9553.78798232 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.134] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 25, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 3 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.77880704403\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0278980322182\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.75090906024\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.77880704403\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=1.77880704403\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}, \"Total Records Seen\": {\"count\": 1, \"max\": 900, \"sum\": 900.0, \"min\": 900}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1566817581.138924, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1566817581.109345}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=10100.996219 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.165] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 25, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 4 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.68650996685\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0622150441632\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.62429483235\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.68650996685\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=1.68650996685\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.9212903678417206, 1.8527486324310303, 1.7788070440292358] min patience loss:1.77880704403 current loss:1.68650996685 absolute loss difference:0.092297077179\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}, \"Total Records Seen\": {\"count\": 1, \"max\": 1200, \"sum\": 1200.0, \"min\": 1200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1566817581.170707, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1566817581.139177}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9478.01053036 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.191] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 20, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 5 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.54469306767\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.11226074025\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.43243232369\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.54469306767\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=1.54469306767\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.8527486324310303, 1.7788070440292358, 1.6865099668502808] min patience loss:1.68650996685 current loss:1.54469306767 absolute loss difference:0.14181689918\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Total Records Seen\": {\"count\": 1, \"max\": 1500, \"sum\": 1500.0, \"min\": 1500}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1566817581.196091, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1566817581.170963}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11885.0234245 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.221] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 24, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 6 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.41973991692\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.166018579155\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.25372129679\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.41973991692\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=1.41973991692\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.7788070440292358, 1.6865099668502808, 1.5446930676698685] min patience loss:1.54469306767 current loss:1.41973991692 absolute loss difference:0.124953150749\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}, \"Total Records Seen\": {\"count\": 1, \"max\": 1800, \"sum\": 1800.0, \"min\": 1800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1566817581.225847, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1566817581.196338}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=10125.2178671 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.249] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 22, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 7 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.29696819186\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.168372420594\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.1285957545\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.29696819186\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=1.29696819186\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.6865099668502808, 1.5446930676698685, 1.419739916920662] min patience loss:1.41973991692 current loss:1.29696819186 absolute loss difference:0.122771725059\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.01s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}, \"Total Records Seen\": {\"count\": 1, \"max\": 2100, \"sum\": 2100.0, \"min\": 2100}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1566817581.255459, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1566817581.226088}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=10170.557473 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.281] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 25, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 8 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.21605342627\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.16753595788\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.04851740599\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.21605342627\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=1.21605342627\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.5446930676698685, 1.419739916920662, 1.2969681918621063] min patience loss:1.29696819186 current loss:1.21605342627 absolute loss difference:0.0809147655964\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}, \"Total Records Seen\": {\"count\": 1, \"max\": 2400, \"sum\": 2400.0, \"min\": 2400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1566817581.286519, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1566817581.255725}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9700.87811948 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.305] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 18, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 9 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.15128317475\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.136409510858\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.01487363875\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.15128317475\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=1.15128317475\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.419739916920662, 1.2969681918621063, 1.2160534262657166] min patience loss:1.21605342627 current loss:1.15128317475 absolute loss difference:0.0647702515125\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Total Records Seen\": {\"count\": 1, \"max\": 2700, \"sum\": 2700.0, \"min\": 2700}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1566817581.310145, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1566817581.286776}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=12778.4218544 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.331] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 21, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 10 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.12199290097\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.119676418602\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 1.00231648982\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.12199290097\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=1.12199290097\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.2969681918621063, 1.2160534262657166, 1.151283174753189] min patience loss:1.15128317475 current loss:1.12199290097 absolute loss difference:0.0292902737856\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"Total Records Seen\": {\"count\": 1, \"max\": 3000, \"sum\": 3000.0, \"min\": 3000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1566817581.335902, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1566817581.310378}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11699.4839657 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.359] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 22, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 11 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.08874198794\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.11334037222\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.975401625037\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.08874198794\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=1.08874198794\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.2160534262657166, 1.151283174753189, 1.121992900967598] min patience loss:1.12199290097 current loss:1.08874198794 absolute loss difference:0.0332509130239\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}, \"Total Records Seen\": {\"count\": 1, \"max\": 3300, \"sum\": 3300.0, \"min\": 3300}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1566817581.364979, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1566817581.336153}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=10359.4609058 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.383] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 18, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 12 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.06264548004\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.114182294346\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.948463216424\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.06264548004\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=1.06264548004\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.151283174753189, 1.121992900967598, 1.0887419879436493] min patience loss:1.08874198794 current loss:1.06264548004 absolute loss difference:0.0260965079069\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}, \"Total Records Seen\": {\"count\": 1, \"max\": 3600, \"sum\": 3600.0, \"min\": 3600}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1566817581.388261, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1566817581.365282}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=12970.7370374 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.409] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 20, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 13 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.02977643907\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.103877420537\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.925899028778\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.02977643907\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=1.02977643907\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.121992900967598, 1.0887419879436493, 1.0626454800367355] min patience loss:1.06264548004 current loss:1.02977643907 absolute loss difference:0.032869040966\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}, \"Total Records Seen\": {\"count\": 1, \"max\": 3900, \"sum\": 3900.0, \"min\": 3900}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1566817581.414918, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1566817581.388494}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11293.833809 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.442] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 26, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 14 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.01652099192\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0867205094546\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.929800480604\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.01652099192\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=1.01652099192\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.0887419879436493, 1.0626454800367355, 1.0297764390707016] min patience loss:1.02977643907 current loss:1.01652099192 absolute loss difference:0.0132554471493\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}, \"Total Records Seen\": {\"count\": 1, \"max\": 4200, \"sum\": 4200.0, \"min\": 4200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1566817581.448666, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1566817581.415551}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=8925.63362298 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.475] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 26, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 15 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.00521732867\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0987647171132\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.906452581286\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.00521732867\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=1.00521732867\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.0626454800367355, 1.0297764390707016, 1.0165209919214249] min patience loss:1.01652099192 current loss:1.00521732867 absolute loss difference:0.0113036632538\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}, \"Total Records Seen\": {\"count\": 1, \"max\": 4500, \"sum\": 4500.0, \"min\": 4500}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1566817581.481196, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1566817581.448935}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9255.81627999 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.502] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 20, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 16 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 1.00833125412\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.099866243545\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.908464983106\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 1.00833125412\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=1.00833125412\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.0297764390707016, 1.0165209919214249, 1.0052173286676407] min patience loss:1.00521732867 current loss:1.00833125412 absolute loss difference:0.003113925457\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}, \"Total Records Seen\": {\"count\": 1, \"max\": 4800, \"sum\": 4800.0, \"min\": 4800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1566817581.503441, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1566817581.481486}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=13595.6521269 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.525] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 21, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 17 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.987614899874\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0831562355161\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.904458656907\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.987614899874\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=0.987614899874\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.0165209919214249, 1.0052173286676407, 1.0083312541246414] min patience loss:1.00521732867 current loss:0.987614899874 absolute loss difference:0.0176024287939\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}, \"Total Records Seen\": {\"count\": 1, \"max\": 5100, \"sum\": 5100.0, \"min\": 5100}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1566817581.530184, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1566817581.503633}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11254.9414574 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.548] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 17, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 18 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.97397261858\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0859348140657\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.888037785888\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.97397261858\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=0.97397261858\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.0052173286676407, 1.0083312541246414, 0.9876148998737335] min patience loss:0.987614899874 current loss:0.97397261858 absolute loss difference:0.0136422812939\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Total Records Seen\": {\"count\": 1, \"max\": 5400, \"sum\": 5400.0, \"min\": 5400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1566817581.552639, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1566817581.530413}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=13435.961175 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.572] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 19, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 19 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.981884896755\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0795275382698\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.902357339859\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.981884896755\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=0.981884896755\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[1.0083312541246414, 0.9876148998737335, 0.9739726185798645] min patience loss:0.97397261858 current loss:0.981884896755 absolute loss difference:0.00791227817535\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}, \"Total Records Seen\": {\"count\": 1, \"max\": 5700, \"sum\": 5700.0, \"min\": 5700}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1566817581.573951, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1566817581.552864}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=14154.8028573 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.594] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 19, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 20 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.958895802498\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0914262768347\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.867469534278\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.958895802498\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=0.958895802498\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9876148998737335, 0.9739726185798645, 0.9818848967552185] min patience loss:0.97397261858 current loss:0.958895802498 absolute loss difference:0.015076816082\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}, \"Total Records Seen\": {\"count\": 1, \"max\": 6000, \"sum\": 6000.0, \"min\": 6000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1566817581.598598, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1566817581.574148}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=12216.0636098 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.619] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 20, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 21 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.972307652235\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0678940815851\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.904413610697\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.972307652235\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=0.972307652235\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9739726185798645, 0.9818848967552185, 0.9588958024978638] min patience loss:0.958895802498 current loss:0.972307652235 absolute loss difference:0.0134118497372\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}, \"Total Records Seen\": {\"count\": 1, \"max\": 6300, \"sum\": 6300.0, \"min\": 6300}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1566817581.62127, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1566817581.598844}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=13300.3319028 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.642] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 21, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 22 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.948612608016\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0813795533031\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.867233030498\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.948612608016\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=0.948612608016\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9818848967552185, 0.9588958024978638, 0.9723076522350311] min patience loss:0.958895802498 current loss:0.948612608016 absolute loss difference:0.0102831944823\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}, \"Total Records Seen\": {\"count\": 1, \"max\": 6600, \"sum\": 6600.0, \"min\": 6600}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1566817581.647085, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1566817581.62152}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11681.3457361 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.667] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 19, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 23 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.951217003167\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0750691853464\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.876147814095\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.951217003167\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=0.951217003167\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9588958024978638, 0.9723076522350311, 0.9486126080155373] min patience loss:0.948612608016 current loss:0.951217003167 absolute loss difference:0.00260439515114\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}, \"Total Records Seen\": {\"count\": 1, \"max\": 6900, \"sum\": 6900.0, \"min\": 6900}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1566817581.6686, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1566817581.647326}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=14028.554546 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.691] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 22, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 24 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.943155199289\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0730064539239\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.87014875561\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.943155199289\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=0.943155199289\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9723076522350311, 0.9486126080155373, 0.9512170031666756] min patience loss:0.948612608016 current loss:0.943155199289 absolute loss difference:0.00545740872622\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}, \"Total Records Seen\": {\"count\": 1, \"max\": 7200, \"sum\": 7200.0, \"min\": 7200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1566817581.695812, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1566817581.668817}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11068.0306455 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.714] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 18, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 25 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.945096820593\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0794164719991\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.865680322051\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.945096820593\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=0.945096820593\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9486126080155373, 0.9512170031666756, 0.9431551992893219] min patience loss:0.943155199289 current loss:0.945096820593 absolute loss difference:0.00194162130356\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}, \"Total Records Seen\": {\"count\": 1, \"max\": 7500, \"sum\": 7500.0, \"min\": 7500}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1566817581.715783, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1566817581.696062}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=15125.5102777 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.737] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 20, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 26 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.931988969445\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0757830948569\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.856205843389\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.931988969445\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=0.931988969445\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9512170031666756, 0.9431551992893219, 0.9450968205928802] min patience loss:0.943155199289 current loss:0.931988969445 absolute loss difference:0.0111662298441\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}, \"Total Records Seen\": {\"count\": 1, \"max\": 7800, \"sum\": 7800.0, \"min\": 7800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1566817581.741426, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1566817581.715984}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11745.3510188 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.763] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 21, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 27 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.919606797397\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0669515840709\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.852655172348\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.919606797397\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=0.919606797397\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9431551992893219, 0.9450968205928802, 0.9319889694452286] min patience loss:0.931988969445 current loss:0.919606797397 absolute loss difference:0.0123821720481\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}, \"Total Records Seen\": {\"count\": 1, \"max\": 8100, \"sum\": 8100.0, \"min\": 8100}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1566817581.767499, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1566817581.741661}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11557.6342644 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.797] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 29, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 28 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.934063062072\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0718522891402\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.86221075058\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.934063062072\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=0.934063062072\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9450968205928802, 0.9319889694452286, 0.9196067973971367] min patience loss:0.919606797397 current loss:0.934063062072 absolute loss difference:0.0144562646747\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}, \"Total Records Seen\": {\"count\": 1, \"max\": 8400, \"sum\": 8400.0, \"min\": 8400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1566817581.799391, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1566817581.767751}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9441.38541651 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.826] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 26, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 29 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.918276876211\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.067405199632\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.850871682167\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.918276876211\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=0.918276876211\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9319889694452286, 0.9196067973971367, 0.9340630620718002] min patience loss:0.919606797397 current loss:0.918276876211 absolute loss difference:0.00132992118597\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}, \"Total Records Seen\": {\"count\": 1, \"max\": 8700, \"sum\": 8700.0, \"min\": 8700}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1566817581.830767, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1566817581.799669}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9607.03640362 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.853] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 22, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 30 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.915696963668\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0711242314428\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.844572715461\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.915696963668\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=0.915696963668\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9196067973971367, 0.9340630620718002, 0.9182768762111664] min patience loss:0.918276876211 current loss:0.915696963668 absolute loss difference:0.0025799125433\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}, \"Total Records Seen\": {\"count\": 1, \"max\": 9000, \"sum\": 9000.0, \"min\": 9000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1566817581.857592, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1566817581.831023}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11245.6873207 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.883] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 25, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 31 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.911205142736\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0560727482662\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.855132363737\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.911205142736\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=0.911205142736\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9340630620718002, 0.9182768762111664, 0.9156969636678696] min patience loss:0.915696963668 current loss:0.911205142736 absolute loss difference:0.00449182093143\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}, \"Total Records Seen\": {\"count\": 1, \"max\": 9300, \"sum\": 9300.0, \"min\": 9300}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1566817581.888363, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1566817581.857806}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9784.45890778 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.908] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 19, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 32 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.892111815512\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0643922248855\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.827719628811\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.892111815512\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=0.892111815512\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9182768762111664, 0.9156969636678696, 0.9112051427364349] min patience loss:0.911205142736 current loss:0.892111815512 absolute loss difference:0.0190933272243\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}, \"Total Records Seen\": {\"count\": 1, \"max\": 9600, \"sum\": 9600.0, \"min\": 9600}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1566817581.912547, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1566817581.888579}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=12462.030306 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.931] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 18, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 33 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.903083220124\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0535191651434\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.849564053118\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.903083220124\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=0.903083220124\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9156969636678696, 0.9112051427364349, 0.8921118155121803] min patience loss:0.892111815512 current loss:0.903083220124 absolute loss difference:0.0109714046121\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}, \"Total Records Seen\": {\"count\": 1, \"max\": 9900, \"sum\": 9900.0, \"min\": 9900}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1566817581.93249, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1566817581.91278}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=15135.5169303 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.956] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 23, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 34 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.898305177689\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0655015883967\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.832803592086\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.898305177689\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=0.898305177689\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.9112051427364349, 0.8921118155121803, 0.9030832201242447] min patience loss:0.892111815512 current loss:0.898305177689 absolute loss difference:0.00619336217642\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}, \"Total Records Seen\": {\"count\": 1, \"max\": 10200, \"sum\": 10200.0, \"min\": 10200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1566817581.957676, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1566817581.932715}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11959.8061021 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:21.979] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 21, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Finished training epoch 35 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) total: 0.888714559376\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) kld: 0.0605547735468\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) recons: 0.828159786761\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Loss (name: value) logppx: 0.888714559376\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=0.888714559376\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] patience losses:[0.8921118155121803, 0.9030832201242447, 0.8983051776885986] min patience loss:0.892111815512 current loss:0.888714559376 absolute loss difference:0.00339725613594\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}, \"Total Records Seen\": {\"count\": 1, \"max\": 10500, \"sum\": 10500.0, \"min\": 10500}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1566817581.983593, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1566817581.95793}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11634.6851595 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:21 INFO 139999912929088] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:22.010] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 25, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Finished training epoch 36 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) total: 0.884239196777\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) kld: 0.0609188466333\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) recons: 0.823320381343\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) logppx: 0.884239196777\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=0.884239196777\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] patience losses:[0.9030832201242447, 0.8983051776885986, 0.8887145593762398] min patience loss:0.888714559376 current loss:0.884239196777 absolute loss difference:0.0044753625989\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}, \"Total Records Seen\": {\"count\": 1, \"max\": 10800, \"sum\": 10800.0, \"min\": 10800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1566817582.015508, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1566817581.983832}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9433.38706171 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:22.040] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 24, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Finished training epoch 37 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) total: 0.877335444093\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) kld: 0.0548328729346\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) recons: 0.822502546012\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) logppx: 0.877335444093\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=0.877335444093\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] patience losses:[0.8983051776885986, 0.8887145593762398, 0.8842391967773438] min patience loss:0.884239196777 current loss:0.877335444093 absolute loss difference:0.00690375268459\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}, \"Total Records Seen\": {\"count\": 1, \"max\": 11100, \"sum\": 11100.0, \"min\": 11100}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1566817582.044489, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1566817582.015756}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=10399.7024621 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:22.063] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 18, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Finished training epoch 38 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) total: 0.877262949944\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) kld: 0.0585466586053\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) recons: 0.818716339767\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) logppx: 0.877262949944\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=0.877262949944\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] patience losses:[0.8887145593762398, 0.8842391967773438, 0.8773354440927505] min patience loss:0.877335444093 current loss:0.877262949944 absolute loss difference:7.24941492081e-05\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}, \"Total Records Seen\": {\"count\": 1, \"max\": 11400, \"sum\": 11400.0, \"min\": 11400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1566817582.067947, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1566817582.044734}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=12857.9434095 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:22.094] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 26, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Finished training epoch 39 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) total: 0.854204826057\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) kld: 0.0519457831979\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) recons: 0.80225905776\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) logppx: 0.854204826057\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=0.854204826057\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] patience losses:[0.8842391967773438, 0.8773354440927505, 0.8772629499435425] min patience loss:0.877262949944 current loss:0.854204826057 absolute loss difference:0.0230581238866\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}, \"Total Records Seen\": {\"count\": 1, \"max\": 11700, \"sum\": 11700.0, \"min\": 11700}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1566817582.099965, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1566817582.068181}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=9403.21488622 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:22.126] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 25, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Finished training epoch 40 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) total: 0.880287945271\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) kld: 0.0550868040882\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) recons: 0.825201109052\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) logppx: 0.880287945271\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=0.880287945271\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] patience losses:[0.8773354440927505, 0.8772629499435425, 0.8542048260569572] min patience loss:0.854204826057 current loss:0.880287945271 absolute loss difference:0.0260831192136\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}, \"Total Records Seen\": {\"count\": 1, \"max\": 12000, \"sum\": 12000.0, \"min\": 12000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1566817582.127219, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1566817582.100205}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11060.3449185 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:22.152] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 24, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Finished training epoch 41 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) total: 0.869323119521\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) kld: 0.057447870262\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) recons: 0.811875276268\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) logppx: 0.869323119521\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=0.869323119521\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] patience losses:[0.8772629499435425, 0.8542048260569572, 0.8802879452705383] min patience loss:0.854204826057 current loss:0.869323119521 absolute loss difference:0.0151182934642\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Timing: train: 0.03s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}, \"Total Records Seen\": {\"count\": 1, \"max\": 12300, \"sum\": 12300.0, \"min\": 12300}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1566817582.153844, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1566817582.127422}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=11302.0506049 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:22.177] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 23, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Finished training epoch 42 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) total: 0.853936836123\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) kld: 0.0513934590854\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) recons: 0.802543364465\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) logppx: 0.853936836123\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=0.853936836123\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] patience losses:[0.8542048260569572, 0.8802879452705383, 0.869323119521141] min patience loss:0.854204826057 current loss:0.853936836123 absolute loss difference:0.000267989933491\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}, \"Total Records Seen\": {\"count\": 1, \"max\": 12600, \"sum\": 12600.0, \"min\": 12600}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1566817582.182038, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1566817582.154105}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=10693.2991136 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] \u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[31m[2019-08-26 11:06:22.202] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 20, \"num_examples\": 2, \"num_bytes\": 68400}\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] # Finished training epoch 43 on 300 examples from 2 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) total: 0.868704222143\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) kld: 0.0541741764173\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) recons: 0.814530096948\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Loss (name: value) logppx: 0.868704222143\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=0.868704222143\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] patience losses:[0.8802879452705383, 0.869323119521141, 0.8539368361234665] min patience loss:0.853936836123 current loss:0.868704222143 absolute loss difference:0.0147673860192\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Total Batches Seen\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}, \"Total Records Seen\": {\"count\": 1, \"max\": 12900, \"sum\": 12900.0, \"min\": 12900}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 300, \"sum\": 300.0, \"min\": 300}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1566817582.203863, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1566817582.182276}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] #throughput_metric: host=algo-1, train throughput=13824.9450646 records/second\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 WARNING 139999912929088] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Best model based on early stopping at epoch 42. Best loss: 0.853936836123\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Topics from epoch:final (num_topics:10) [, tu 0.23]:\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 0 1 2 3 4 5 7 8 6 9 10 44 29 22 49 35 24 41 26 40\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 0 1 2 3 4 5 7 6 8 9 11 12 13 31 37 32 47 19 29 18\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 0 1 2 4 3 5 8 6 9 7 10 11 32 33 43 25 16 19 46 12\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 0 1 2 3 4 5 7 9 6 10 11 48 31 25 34 28 8 46 44 21\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 1 0 2 3 4 5 6 7 9 8 10 26 11 37 22 31 29 24 41 48\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 0 1 3 2 4 5 6 7 8 10 9 16 41 49 15 14 17 28 42 32\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 1 0 2 3 4 6 5 7 8 9 10 44 25 47 36 27 11 41 15 29\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 0 1 2 3 4 5 7 38 8 29 24 47 19 31 6 9 14 30 25 43\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 0 1 2 3 4 7 5 6 9 8 40 27 36 31 15 37 29 33 48 20\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] 0 1 2 3 4 5 6 8 7 12 24 19 44 10 36 26 37 40 9 48\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Saved checkpoint to \"/tmp/tmpH5TaVh/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[08/26/2019 11:06:22 INFO 139999912929088] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1323.9641189575195, \"sum\": 1323.9641189575195, \"min\": 1323.9641189575195}, \"finalize.time\": {\"count\": 1, \"max\": 45.330047607421875, \"sum\": 45.330047607421875, \"min\": 45.330047607421875}, \"initialize.time\": {\"count\": 1, \"max\": 17.72594451904297, \"sum\": 17.72594451904297, \"min\": 17.72594451904297}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.5360584259033203, \"sum\": 2.5360584259033203, \"min\": 2.5360584259033203}, \"setuptime\": {\"count\": 1, \"max\": 39.34216499328613, \"sum\": 39.34216499328613, \"min\": 39.34216499328613}, \"early_stop.time\": {\"count\": 43, \"max\": 5.00178337097168, \"sum\": 117.28906631469727, \"min\": 0.14090538024902344}, \"update.time\": {\"count\": 43, \"max\": 60.9891414642334, \"sum\": 1169.726848602295, \"min\": 19.56009864807129}, \"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1566817582.252765, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1566817580.99585}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-26 11:06:30 Uploading - Uploading generated training model\n",
      "2019-08-26 11:06:30 Completed - Training job completed\n",
      "Billable seconds: 60\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "ntm.set_hyperparameters(num_topics=num_topics,\n",
    "                        feature_dim=vocabulary_size)\n",
    "\n",
    "ntm.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#real time to get predictor from endpoint\n",
    "ntm_predictor=sagemaker.predictor.RealTimePredictor(endpoint=\"ntm-endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ntm_predictor.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(listOfStr):\n",
    "    value=max(listOfStr)\n",
    "    dictOfWords = { i : listOfStr[i] for i in range(0, len(listOfStr) ) }\n",
    "    for index, num in dictOfWords.items():\n",
    "        if num == value:\n",
    "            return index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_list=[]\n",
    "for lst in range(len(predictions)):\n",
    "    cate_list.append(get_category(predictions[lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1815"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=pd.Series(cate_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\"comment\":df_mutilabel['comment'].tolist(),\"category\":s.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>必要 書類 満足</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>迅速 手続き</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ありません</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>満足</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>丁寧 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>担当者 アドバイス 的確 迅速</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>スムーズ 満足</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>処理</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>必要 書類 方</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>信用 人 楽</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>変更 手続き</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>速やか 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>簡単</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>丁寧</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>書類 簡易 希望</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>セールスマン 親切</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>手続き の</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>普通</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>プラン</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>迅速 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>スムーズ 手続き</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>代理店 方 手続き 実施</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>面倒 手続き 感謝</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>氏名 変更 手続き スムーズ</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>問題 スムーズ</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>丁寧 説明 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>担当 方 いろいろ 提案 迅速 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>最初 丁寧 説明 感謝</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>丁寧 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>問題</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>生活 必要 更新 金銭的 非常</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>迅速</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>丁寧</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>敏速 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>スムーズ</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>迅速 対応 大変</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>非常</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>インターネット の</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>担当者 アフターサービス</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>親切 丁寧 対応 大変</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>ところ 書類 記入 手続き</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>こと</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>最初 時 こちら 不備 迅速 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>何 社 手続き 印象</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>丁寧 迅速 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>内容 説明 納得</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>お願い</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>説明 適切 アドバイス</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>手軽 簡単</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>遠方 何度 足 感謝</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>急 出費</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>スムーズ</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>どこ 一 箇所 うち かた 箇所 時間</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>不備 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>希望 アドバイス 移転先 遠方 来訪 大変 満足</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>係員 方 話して 丁寧 親切 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>支払い</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>丁寧 対応</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>付箋 お手数</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>スムーズ</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       comment  category\n",
       "0                     必要 書類 満足         7\n",
       "4                       迅速 手続き         7\n",
       "6                        ありません         7\n",
       "9                           満足         7\n",
       "10                       丁寧 対応         7\n",
       "11             担当者 アドバイス 的確 迅速         7\n",
       "13                     スムーズ 満足         7\n",
       "14                          処理         7\n",
       "18                     必要 書類 方         7\n",
       "22                      信用 人 楽         7\n",
       "23                      変更 手続き         7\n",
       "25                      速やか 対応         7\n",
       "26                          簡単         7\n",
       "28                          丁寧         7\n",
       "31                    書類 簡易 希望         7\n",
       "33                   セールスマン 親切         7\n",
       "34                       手続き の         7\n",
       "36                          普通         7\n",
       "38                         プラン         7\n",
       "40                       迅速 対応         7\n",
       "41                    スムーズ 手続き         7\n",
       "44                代理店 方 手続き 実施         7\n",
       "45                   面倒 手続き 感謝         7\n",
       "46              氏名 変更 手続き スムーズ         7\n",
       "48                     問題 スムーズ         7\n",
       "50                    丁寧 説明 対応         7\n",
       "52          担当 方 いろいろ 提案 迅速 対応         7\n",
       "53                 最初 丁寧 説明 感謝         7\n",
       "55                       丁寧 対応         7\n",
       "56                          問題         7\n",
       "...                        ...       ...\n",
       "1748           生活 必要 更新 金銭的 非常         7\n",
       "1749                        迅速         7\n",
       "1750                        丁寧         7\n",
       "1751                     敏速 対応         7\n",
       "1752                      スムーズ         7\n",
       "1753                  迅速 対応 大変         7\n",
       "1756                        非常         7\n",
       "1757                 インターネット の         7\n",
       "1763              担当者 アフターサービス         7\n",
       "1766               親切 丁寧 対応 大変         7\n",
       "1767             ところ 書類 記入 手続き         7\n",
       "1769                        こと         7\n",
       "1772         最初 時 こちら 不備 迅速 対応         7\n",
       "1776                何 社 手続き 印象         7\n",
       "1779                  丁寧 迅速 対応         7\n",
       "1780                  内容 説明 納得         7\n",
       "1785                       お願い         7\n",
       "1789               説明 適切 アドバイス         7\n",
       "1793                     手軽 簡単         7\n",
       "1794                遠方 何度 足 感謝         7\n",
       "1795                      急 出費         7\n",
       "1797                      スムーズ         7\n",
       "1798       どこ 一 箇所 うち かた 箇所 時間         7\n",
       "1799                     不備 対応         7\n",
       "1800  希望 アドバイス 移転先 遠方 来訪 大変 満足         7\n",
       "1801         係員 方 話して 丁寧 親切 対応         7\n",
       "1803                       支払い         7\n",
       "1808                     丁寧 対応         7\n",
       "1810                    付箋 お手数         7\n",
       "1811                      スムーズ         7\n",
       "\n",
       "[1056 rows x 2 columns]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['category']==7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel=df_mutilabel['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel['category']=pd.Series(cate_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
